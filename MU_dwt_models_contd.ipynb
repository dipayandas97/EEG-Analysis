{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MU_dwt_models_contd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipayandas97/EEG-Analysis/blob/master/MU_dwt_models_contd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ4l-QT--2wd",
        "colab_type": "code",
        "outputId": "30d41166-c7e4-48cf-f720-7a92ea5f32a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "data_path = '/content/drive/My Drive/AI_datasets/MW/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybtBDDkg_LDs",
        "colab_type": "code",
        "outputId": "f7a2ef13-f380-4636-d844-c5cf1a619708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle \n",
        "\n",
        "w = open(data_path+'MU_X_real', 'rb')\n",
        "X_real_series = pickle.load(w)\n",
        "w.close()\n",
        "len(X_real_series)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40983"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAhNrkYB_gYz",
        "colab_type": "code",
        "outputId": "3c713d06-c9cd-4a52-8686-dc8fc6e3e3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle \n",
        "\n",
        "q = open(data_path+'MU_final_Y', 'rb')\n",
        "Y_hot = pickle.load(q)\n",
        "q.close()\n",
        "Y_hot.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40983, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbl5E7Ep_i_t",
        "colab_type": "code",
        "outputId": "7807c855-d902-48b7-fd85-49e49bf49395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "#create real series of 512 time steps\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_real_padded = np.zeros((len(X_real_series),4,512))\n",
        "\n",
        "for s in range(len(X_real_series)):\n",
        "  X_real_padded[s] = pad_sequences(X_real_series[s], padding='post', maxlen = 512, value=np.mean(X_real_series[s]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enILeCz9_t7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pywt\n",
        "import itertools\n",
        "import cv2\n",
        "\n",
        "lv = 8\n",
        "max_time_steps = 256\n",
        "data = []\n",
        "\n",
        "for s in range(X_real_padded.shape[0]):\n",
        "  sample_=[]\n",
        "  for e in range(4):\n",
        "    dwt = pywt.wavedec(X_real_padded[s][e], 'db1', level=lv)\n",
        "    \n",
        "    img = []\n",
        "    for i in range(len(dwt)): # 9\n",
        "      t = [x for x in list(itertools.chain.from_iterable(itertools.repeat(x, max_time_steps//len(dwt[i])) for x in dwt[i]))]\n",
        "      img.append(np.asarray(t))\n",
        "    sample_.append(cv2.resize(np.asarray(img[1:]), (32,32)))\n",
        "  sample_ = np.asarray(sample_) + abs(np.min(np.asarray(sample_)))\n",
        "  sample_ = sample_ / np.max(sample_)\n",
        "  data.append(sample_)\n",
        "  print(s)\n",
        "data = np.asarray(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyOnRZdfAFf4",
        "colab_type": "code",
        "outputId": "85c3e034-fec1-47de-9703-3cbbdcde1fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40983, 4, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7ObER2kAHJ2",
        "colab_type": "code",
        "outputId": "ce5b026f-806c-4ee3-ad5a-13719fc6e532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.imshow(data[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6bf9ee0cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD-qHkrgAKBm",
        "colab_type": "code",
        "outputId": "08f3ecf5-3569-468e-923f-db7ea591328e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print('Max: ', np.max(data))\n",
        "print('Min: ', np.min(data))\n",
        "print('Mean: ', np.mean(data))\n",
        "print('Std: ', np.std(data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max:  1.0\n",
            "Min:  0.0\n",
            "Mean:  0.5040510697256819\n",
            "Std:  0.17850077831510153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U34-kngLKORg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle \n",
        "\n",
        "e = open(data_path+'MU_32x32_dwt_X', 'wb')\n",
        "pickle.dump(data, e)\n",
        "e.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC4ehY-oMdPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, Y_hot, test_size=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knUiEDGmDCoc",
        "colab_type": "code",
        "outputId": "fb7f3367-d352-49c8-893f-56b38cc77934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "v = Input(shape=(4, 32, 32))\n",
        "x = Conv2D(50, (3,3), activation = 'relu', data_format='channels_first')(v)\n",
        "x = MaxPooling2D(2,2)(x)\n",
        "x = Conv2D(25, (3,3), activation = 'relu')(x)\n",
        "x = MaxPooling2D(2,2)(x)\n",
        "x = Conv2D(20, (2,2), activation = 'relu')(x)\n",
        "x = MaxPooling2D(2,2)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(11, activation = 'softmax')(x)\n",
        "net = Model(v,x)\n",
        "net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "net.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 4, 32, 32)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 50, 30, 30)        1850      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 25, 15, 30)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 23, 13, 25)        6775      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 11, 6, 25)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 10, 5, 20)         2020      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 5, 2, 20)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                6432      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 17,440\n",
            "Trainable params: 17,440\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvrJ1UWlIOiQ",
        "colab_type": "code",
        "outputId": "5af111b0-3d8d-42cd-e74a-e2e99eec414b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "net.fit(data, Y_hot, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32786 samples, validate on 8197 samples\n",
            "Epoch 1/50\n",
            "32786/32786 [==============================] - 11s 341us/step - loss: 2.1825 - acc: 0.1724 - val_loss: 0.4280 - val_acc: 0.9589\n",
            "Epoch 2/50\n",
            "32786/32786 [==============================] - 11s 342us/step - loss: 2.1705 - acc: 0.1779 - val_loss: 0.2568 - val_acc: 0.9894\n",
            "Epoch 3/50\n",
            "32786/32786 [==============================] - 11s 342us/step - loss: 2.1648 - acc: 0.1774 - val_loss: 0.3321 - val_acc: 0.9652\n",
            "Epoch 4/50\n",
            "32786/32786 [==============================] - 11s 335us/step - loss: 2.1582 - acc: 0.1794 - val_loss: 0.5375 - val_acc: 0.9405\n",
            "Epoch 5/50\n",
            "32786/32786 [==============================] - 11s 340us/step - loss: 2.1544 - acc: 0.1827 - val_loss: 0.3550 - val_acc: 0.9611\n",
            "Epoch 6/50\n",
            "27168/32786 [=======================>......] - ETA: 1s - loss: 2.1516 - acc: 0.1833"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-6af778147a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# For backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCpLN9geLXTN",
        "colab_type": "code",
        "outputId": "3f16c1ea-9c3e-4623-dd8c-737b14c3416b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40983, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGUehNWuIbcw",
        "colab_type": "code",
        "outputId": "5f740888-d580-4eae-feab-4fc93f88df95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "chunk =X_test\n",
        "pred = net.predict(chunk)\n",
        "true = y_test\n",
        "pred_ = [np.argmax(x) for x in pred]\n",
        "true_ = [np.argmax(x) for x in true]\n",
        "accuracy_score(true_, pred_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3691744611630744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7a2P3u0M8ZN",
        "colab_type": "code",
        "outputId": "32bdbc15-769a-4070-a137-2b882ee948df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "v = Input(shape=(4, 32, 32))\n",
        "x = Conv2D(50, (3,3), activation = 'relu', data_format='channels_first')(v)\n",
        "x = Dropout(0.2)(x)\n",
        "x = MaxPooling2D(2,2)(x)\n",
        "x = Conv2D(25, (3,3), activation = 'relu')(x)\n",
        "x = MaxPooling2D(2,2)(x)\n",
        "x = Conv2D(20, (2,2), activation = 'relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = MaxPooling2D(2,2)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(11, activation = 'softmax')(x)\n",
        "net2 = Model(v,x)\n",
        "net2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "net2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 4, 32, 32)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 50, 30, 30)        1850      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50, 30, 30)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 25, 15, 30)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 23, 13, 25)        6775      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 11, 6, 25)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 10, 5, 20)         2020      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 10, 5, 20)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 5, 2, 20)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 32)                3232      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 34,340\n",
            "Trainable params: 34,340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9ICUE1aNATZ",
        "colab_type": "code",
        "outputId": "06c81fce-9b32-4750-c1c6-8bf47db0319d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net2.fit(X_train, y_train, epochs=50, batch_size=50, shuffle=True, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32786 samples, validate on 8197 samples\n",
            "Epoch 1/50\n",
            "32786/32786 [==============================] - 11s 333us/step - loss: 2.0972 - acc: 0.2907 - val_loss: 1.8725 - val_acc: 0.3323\n",
            "Epoch 2/50\n",
            "32786/32786 [==============================] - 9s 287us/step - loss: 1.8391 - acc: 0.3297 - val_loss: 1.8558 - val_acc: 0.3326\n",
            "Epoch 3/50\n",
            "32786/32786 [==============================] - 10s 293us/step - loss: 1.8134 - acc: 0.3315 - val_loss: 1.8132 - val_acc: 0.3341\n",
            "Epoch 4/50\n",
            "32786/32786 [==============================] - 9s 289us/step - loss: 1.7945 - acc: 0.3349 - val_loss: 1.8016 - val_acc: 0.3329\n",
            "Epoch 5/50\n",
            "32786/32786 [==============================] - 9s 288us/step - loss: 1.7812 - acc: 0.3359 - val_loss: 1.7815 - val_acc: 0.3366\n",
            "Epoch 6/50\n",
            "32786/32786 [==============================] - 10s 291us/step - loss: 1.7710 - acc: 0.3397 - val_loss: 1.7684 - val_acc: 0.3359\n",
            "Epoch 7/50\n",
            "32786/32786 [==============================] - 9s 284us/step - loss: 1.7686 - acc: 0.3368 - val_loss: 1.7623 - val_acc: 0.3328\n",
            "Epoch 8/50\n",
            "32786/32786 [==============================] - 10s 291us/step - loss: 1.7576 - acc: 0.3389 - val_loss: 1.7600 - val_acc: 0.3337\n",
            "Epoch 9/50\n",
            "32786/32786 [==============================] - 9s 288us/step - loss: 1.7604 - acc: 0.3364 - val_loss: 1.7699 - val_acc: 0.3312\n",
            "Epoch 10/50\n",
            "32786/32786 [==============================] - 10s 291us/step - loss: 1.7535 - acc: 0.3393 - val_loss: 1.7474 - val_acc: 0.3404\n",
            "Epoch 11/50\n",
            "32786/32786 [==============================] - 9s 285us/step - loss: 1.7456 - acc: 0.3390 - val_loss: 1.7582 - val_acc: 0.3362\n",
            "Epoch 12/50\n",
            "32786/32786 [==============================] - 9s 289us/step - loss: 1.7434 - acc: 0.3421 - val_loss: 1.7394 - val_acc: 0.3401\n",
            "Epoch 13/50\n",
            "32786/32786 [==============================] - 9s 284us/step - loss: 1.7411 - acc: 0.3417 - val_loss: 1.7437 - val_acc: 0.3381\n",
            "Epoch 14/50\n",
            "32786/32786 [==============================] - 9s 286us/step - loss: 1.7390 - acc: 0.3410 - val_loss: 1.7490 - val_acc: 0.3426\n",
            "Epoch 15/50\n",
            "32786/32786 [==============================] - 9s 286us/step - loss: 1.7318 - acc: 0.3418 - val_loss: 1.7440 - val_acc: 0.3420\n",
            "Epoch 16/50\n",
            "32786/32786 [==============================] - 10s 294us/step - loss: 1.7376 - acc: 0.3424 - val_loss: 1.7463 - val_acc: 0.3418\n",
            "Epoch 17/50\n",
            "32786/32786 [==============================] - 10s 301us/step - loss: 1.7309 - acc: 0.3477 - val_loss: 1.7500 - val_acc: 0.3413\n",
            "Epoch 18/50\n",
            "32786/32786 [==============================] - 10s 295us/step - loss: 1.7272 - acc: 0.3449 - val_loss: 1.7412 - val_acc: 0.3349\n",
            "Epoch 19/50\n",
            "32786/32786 [==============================] - 9s 287us/step - loss: 1.7277 - acc: 0.3445 - val_loss: 1.7412 - val_acc: 0.3396\n",
            "Epoch 20/50\n",
            "32786/32786 [==============================] - 10s 294us/step - loss: 1.7256 - acc: 0.3458 - val_loss: 1.7430 - val_acc: 0.3362\n",
            "Epoch 21/50\n",
            "32786/32786 [==============================] - 10s 294us/step - loss: 1.7231 - acc: 0.3465 - val_loss: 1.7390 - val_acc: 0.3399\n",
            "Epoch 22/50\n",
            "32786/32786 [==============================] - 10s 299us/step - loss: 1.7204 - acc: 0.3473 - val_loss: 1.7393 - val_acc: 0.3418\n",
            "Epoch 23/50\n",
            "32786/32786 [==============================] - 9s 289us/step - loss: 1.7185 - acc: 0.3491 - val_loss: 1.7444 - val_acc: 0.3344\n",
            "Epoch 24/50\n",
            "32786/32786 [==============================] - 10s 291us/step - loss: 1.7192 - acc: 0.3488 - val_loss: 1.7431 - val_acc: 0.3343\n",
            "Epoch 25/50\n",
            "32786/32786 [==============================] - 10s 291us/step - loss: 1.7128 - acc: 0.3491 - val_loss: 1.7406 - val_acc: 0.3428\n",
            "Epoch 26/50\n",
            "32786/32786 [==============================] - 10s 291us/step - loss: 1.7134 - acc: 0.3496 - val_loss: 1.7378 - val_acc: 0.3415\n",
            "Epoch 27/50\n",
            "32786/32786 [==============================] - 9s 286us/step - loss: 1.7135 - acc: 0.3502 - val_loss: 1.7365 - val_acc: 0.3367\n",
            "Epoch 28/50\n",
            "32786/32786 [==============================] - 9s 286us/step - loss: 1.7070 - acc: 0.3557 - val_loss: 1.7335 - val_acc: 0.3396\n",
            "Epoch 29/50\n",
            "32786/32786 [==============================] - 10s 290us/step - loss: 1.7051 - acc: 0.3527 - val_loss: 1.7383 - val_acc: 0.3395\n",
            "Epoch 30/50\n",
            "32786/32786 [==============================] - 9s 286us/step - loss: 1.7040 - acc: 0.3529 - val_loss: 1.7377 - val_acc: 0.3374\n",
            "Epoch 31/50\n",
            "32786/32786 [==============================] - 10s 292us/step - loss: 1.7064 - acc: 0.3560 - val_loss: 1.7388 - val_acc: 0.3396\n",
            "Epoch 32/50\n",
            "32786/32786 [==============================] - 9s 288us/step - loss: 1.7055 - acc: 0.3553 - val_loss: 1.7500 - val_acc: 0.3400\n",
            "Epoch 33/50\n",
            "32786/32786 [==============================] - 9s 286us/step - loss: 1.7001 - acc: 0.3565 - val_loss: 1.7358 - val_acc: 0.3423\n",
            "Epoch 34/50\n",
            "32786/32786 [==============================] - 9s 286us/step - loss: 1.6976 - acc: 0.3599 - val_loss: 1.7399 - val_acc: 0.3444\n",
            "Epoch 35/50\n",
            "32786/32786 [==============================] - 9s 284us/step - loss: 1.7002 - acc: 0.3580 - val_loss: 1.7428 - val_acc: 0.3442\n",
            "Epoch 36/50\n",
            "32786/32786 [==============================] - 9s 282us/step - loss: 1.6947 - acc: 0.3596 - val_loss: 1.7478 - val_acc: 0.3410\n",
            "Epoch 37/50\n",
            "32786/32786 [==============================] - 9s 282us/step - loss: 1.6916 - acc: 0.3640 - val_loss: 1.7408 - val_acc: 0.3448\n",
            "Epoch 38/50\n",
            "32786/32786 [==============================] - 9s 280us/step - loss: 1.6922 - acc: 0.3647 - val_loss: 1.7487 - val_acc: 0.3373\n",
            "Epoch 39/50\n",
            "32786/32786 [==============================] - 9s 285us/step - loss: 1.6884 - acc: 0.3638 - val_loss: 1.7417 - val_acc: 0.3398\n",
            "Epoch 40/50\n",
            "32786/32786 [==============================] - 9s 284us/step - loss: 1.6844 - acc: 0.3653 - val_loss: 1.7538 - val_acc: 0.3404\n",
            "Epoch 41/50\n",
            "32786/32786 [==============================] - 10s 293us/step - loss: 1.6863 - acc: 0.3656 - val_loss: 1.7571 - val_acc: 0.3381\n",
            "Epoch 42/50\n",
            "32786/32786 [==============================] - 9s 287us/step - loss: 1.6846 - acc: 0.3687 - val_loss: 1.7447 - val_acc: 0.3388\n",
            "Epoch 43/50\n",
            "32786/32786 [==============================] - 10s 298us/step - loss: 1.6820 - acc: 0.3678 - val_loss: 1.7546 - val_acc: 0.3396\n",
            "Epoch 44/50\n",
            "32786/32786 [==============================] - 9s 288us/step - loss: 1.6791 - acc: 0.3675 - val_loss: 1.7553 - val_acc: 0.3379\n",
            "Epoch 45/50\n",
            "32786/32786 [==============================] - 10s 291us/step - loss: 1.6775 - acc: 0.3672 - val_loss: 1.7606 - val_acc: 0.3402\n",
            "Epoch 46/50\n",
            "32786/32786 [==============================] - 10s 290us/step - loss: 1.6754 - acc: 0.3702 - val_loss: 1.7562 - val_acc: 0.3384\n",
            "Epoch 47/50\n",
            "32786/32786 [==============================] - 10s 291us/step - loss: 1.6782 - acc: 0.3690 - val_loss: 1.7531 - val_acc: 0.3374\n",
            "Epoch 48/50\n",
            "32786/32786 [==============================] - 10s 290us/step - loss: 1.6769 - acc: 0.3712 - val_loss: 1.7544 - val_acc: 0.3417\n",
            "Epoch 49/50\n",
            "32786/32786 [==============================] - 10s 296us/step - loss: 1.6734 - acc: 0.3682 - val_loss: 1.7610 - val_acc: 0.3432\n",
            "Epoch 50/50\n",
            "32786/32786 [==============================] - 9s 288us/step - loss: 1.6705 - acc: 0.3712 - val_loss: 1.7546 - val_acc: 0.3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6b32da7ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w351-zVGNKbj",
        "colab_type": "code",
        "outputId": "7ea82563-baee-4d15-8a43-e5f41059ccd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "chunk =X_test\n",
        "pred = net2.predict(chunk)\n",
        "true = y_test\n",
        "pred_ = [np.argmax(x) for x in pred]\n",
        "true_ = [np.argmax(x) for x in true]\n",
        "accuracy_score(true_, pred_)\n",
        "#confusion_matrix(true_, pred_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.345370257411248"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxUDrmaeOu_0",
        "colab_type": "code",
        "outputId": "6c9cf5cb-596b-4a76-894f-ec596d2d0ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "chunk =X_test\n",
        "pred = net2.predict(chunk)\n",
        "true = y_test\n",
        "pred_ = [np.argmax(x) for x in pred]\n",
        "true_ = [np.argmax(x) for x in true]\n",
        "accuracy_score(true_, pred_)\n",
        "#confusion_matrix(true_, pred_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33963645236061973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L85XilqFU8Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_ = [x.reshape(1, x.shape[0], x.shape[1], x.shape[2]) for x in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHEU1I_7WO4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.asarray(data_), Y_hot, test_size=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBwlpOejUs9F",
        "colab_type": "code",
        "outputId": "a5a9e817-5359-44d4-aa9f-f79c72e68be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "v = Input(shape=(1, 4, 32, 32))\n",
        "x = Conv3D(50, (2,3,3), activation = 'relu', data_format='channels_first')(v)\n",
        "x = Conv3D(25, 2, activation = 'relu')(x)\n",
        "x = Conv3D(20, 2, activation = 'relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(11, activation = 'softmax')(x)\n",
        "net3 = Model(v,x)\n",
        "net3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "net3.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        (None, 1, 4, 32, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_17 (Conv3D)           (None, 50, 3, 30, 30)     950       \n",
            "_________________________________________________________________\n",
            "conv3d_18 (Conv3D)           (None, 49, 2, 29, 25)     6025      \n",
            "_________________________________________________________________\n",
            "conv3d_19 (Conv3D)           (None, 48, 1, 28, 20)     4020      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 48, 1, 28, 20)     0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 26880)             0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 100)               2688100   \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 32)                3232      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 2,702,690\n",
            "Trainable params: 2,702,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeP8EWfyU1_k",
        "colab_type": "code",
        "outputId": "b143815f-0e47-4381-f5ac-b9e75ff786bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net3.fit(X_train,y_train,epochs=50,batch_size=50,shuffle=True, validation_data=(X_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32786 samples, validate on 8197 samples\n",
            "Epoch 1/50\n",
            "32786/32786 [==============================] - 26s 799us/step - loss: 1.8973 - acc: 0.3218 - val_loss: 1.8027 - val_acc: 0.3384\n",
            "Epoch 2/50\n",
            "32786/32786 [==============================] - 24s 737us/step - loss: 1.7965 - acc: 0.3362 - val_loss: 1.7841 - val_acc: 0.3381\n",
            "Epoch 3/50\n",
            "32786/32786 [==============================] - 24s 734us/step - loss: 1.7763 - acc: 0.3384 - val_loss: 1.7596 - val_acc: 0.3359\n",
            "Epoch 4/50\n",
            "32786/32786 [==============================] - 24s 727us/step - loss: 1.7590 - acc: 0.3397 - val_loss: 1.7482 - val_acc: 0.3394\n",
            "Epoch 5/50\n",
            "32786/32786 [==============================] - 24s 723us/step - loss: 1.7453 - acc: 0.3422 - val_loss: 1.7425 - val_acc: 0.3374\n",
            "Epoch 6/50\n",
            "32786/32786 [==============================] - 24s 728us/step - loss: 1.7417 - acc: 0.3452 - val_loss: 1.7537 - val_acc: 0.3417\n",
            "Epoch 7/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.7353 - acc: 0.3456 - val_loss: 1.7651 - val_acc: 0.3378\n",
            "Epoch 8/50\n",
            "32786/32786 [==============================] - 24s 723us/step - loss: 1.7260 - acc: 0.3516 - val_loss: 1.7413 - val_acc: 0.3423\n",
            "Epoch 9/50\n",
            "32786/32786 [==============================] - 24s 723us/step - loss: 1.7210 - acc: 0.3519 - val_loss: 1.7348 - val_acc: 0.3388\n",
            "Epoch 10/50\n",
            "32786/32786 [==============================] - 24s 722us/step - loss: 1.7178 - acc: 0.3531 - val_loss: 1.7321 - val_acc: 0.3418\n",
            "Epoch 11/50\n",
            "32786/32786 [==============================] - 24s 727us/step - loss: 1.7100 - acc: 0.3571 - val_loss: 1.7554 - val_acc: 0.3366\n",
            "Epoch 12/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.7004 - acc: 0.3604 - val_loss: 1.7484 - val_acc: 0.3390\n",
            "Epoch 13/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.6950 - acc: 0.3637 - val_loss: 1.7427 - val_acc: 0.3365\n",
            "Epoch 14/50\n",
            "32786/32786 [==============================] - 24s 726us/step - loss: 1.6847 - acc: 0.3679 - val_loss: 1.7636 - val_acc: 0.3356\n",
            "Epoch 15/50\n",
            "32786/32786 [==============================] - 24s 730us/step - loss: 1.6788 - acc: 0.3717 - val_loss: 1.7704 - val_acc: 0.3357\n",
            "Epoch 16/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.6626 - acc: 0.3804 - val_loss: 1.7802 - val_acc: 0.3395\n",
            "Epoch 17/50\n",
            "32786/32786 [==============================] - 24s 724us/step - loss: 1.6543 - acc: 0.3846 - val_loss: 1.7798 - val_acc: 0.3372\n",
            "Epoch 18/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.6377 - acc: 0.3922 - val_loss: 1.7953 - val_acc: 0.3363\n",
            "Epoch 19/50\n",
            "32786/32786 [==============================] - 24s 732us/step - loss: 1.6247 - acc: 0.3995 - val_loss: 1.8215 - val_acc: 0.3420\n",
            "Epoch 20/50\n",
            "32786/32786 [==============================] - 24s 733us/step - loss: 1.6076 - acc: 0.4066 - val_loss: 1.8320 - val_acc: 0.3387\n",
            "Epoch 21/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.5891 - acc: 0.4142 - val_loss: 1.8473 - val_acc: 0.3367\n",
            "Epoch 22/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.5692 - acc: 0.4227 - val_loss: 1.8603 - val_acc: 0.3396\n",
            "Epoch 23/50\n",
            "32786/32786 [==============================] - 24s 730us/step - loss: 1.5497 - acc: 0.4316 - val_loss: 1.9142 - val_acc: 0.3368\n",
            "Epoch 24/50\n",
            "32786/32786 [==============================] - 24s 733us/step - loss: 1.5268 - acc: 0.4438 - val_loss: 1.9217 - val_acc: 0.3378\n",
            "Epoch 25/50\n",
            "32786/32786 [==============================] - 24s 724us/step - loss: 1.5011 - acc: 0.4520 - val_loss: 1.9493 - val_acc: 0.3372\n",
            "Epoch 26/50\n",
            "32786/32786 [==============================] - 24s 731us/step - loss: 1.4754 - acc: 0.4638 - val_loss: 2.0302 - val_acc: 0.3349\n",
            "Epoch 27/50\n",
            "32786/32786 [==============================] - 24s 726us/step - loss: 1.4505 - acc: 0.4746 - val_loss: 2.0257 - val_acc: 0.3337\n",
            "Epoch 28/50\n",
            "32786/32786 [==============================] - 24s 722us/step - loss: 1.4220 - acc: 0.4850 - val_loss: 2.0874 - val_acc: 0.3399\n",
            "Epoch 29/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.4016 - acc: 0.4940 - val_loss: 2.1034 - val_acc: 0.3354\n",
            "Epoch 30/50\n",
            "32786/32786 [==============================] - 24s 731us/step - loss: 1.3736 - acc: 0.5030 - val_loss: 2.1504 - val_acc: 0.3302\n",
            "Epoch 31/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.3467 - acc: 0.5121 - val_loss: 2.2399 - val_acc: 0.3337\n",
            "Epoch 32/50\n",
            "32786/32786 [==============================] - 24s 730us/step - loss: 1.3238 - acc: 0.5229 - val_loss: 2.2742 - val_acc: 0.3366\n",
            "Epoch 33/50\n",
            "32786/32786 [==============================] - 24s 727us/step - loss: 1.2932 - acc: 0.5343 - val_loss: 2.3085 - val_acc: 0.3390\n",
            "Epoch 34/50\n",
            "32786/32786 [==============================] - 24s 724us/step - loss: 1.2748 - acc: 0.5408 - val_loss: 2.3310 - val_acc: 0.3356\n",
            "Epoch 35/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.2504 - acc: 0.5510 - val_loss: 2.3727 - val_acc: 0.3330\n",
            "Epoch 36/50\n",
            "32786/32786 [==============================] - 24s 727us/step - loss: 1.2200 - acc: 0.5602 - val_loss: 2.4296 - val_acc: 0.3373\n",
            "Epoch 37/50\n",
            "32786/32786 [==============================] - 24s 728us/step - loss: 1.2033 - acc: 0.5673 - val_loss: 2.4424 - val_acc: 0.3383\n",
            "Epoch 38/50\n",
            "32786/32786 [==============================] - 24s 722us/step - loss: 1.1850 - acc: 0.5741 - val_loss: 2.4403 - val_acc: 0.3384\n",
            "Epoch 39/50\n",
            "32786/32786 [==============================] - 24s 723us/step - loss: 1.1534 - acc: 0.5860 - val_loss: 2.4942 - val_acc: 0.3404\n",
            "Epoch 40/50\n",
            "32786/32786 [==============================] - 24s 734us/step - loss: 1.1350 - acc: 0.5959 - val_loss: 2.6320 - val_acc: 0.3318\n",
            "Epoch 41/50\n",
            "32786/32786 [==============================] - 24s 725us/step - loss: 1.1071 - acc: 0.6036 - val_loss: 2.7394 - val_acc: 0.3326\n",
            "Epoch 42/50\n",
            "32786/32786 [==============================] - 24s 726us/step - loss: 1.0878 - acc: 0.6085 - val_loss: 2.6936 - val_acc: 0.3324\n",
            "Epoch 43/50\n",
            "32786/32786 [==============================] - 24s 729us/step - loss: 1.0776 - acc: 0.6164 - val_loss: 2.6969 - val_acc: 0.3339\n",
            "Epoch 44/50\n",
            "32786/32786 [==============================] - 24s 731us/step - loss: 1.0564 - acc: 0.6230 - val_loss: 2.7205 - val_acc: 0.3368\n",
            "Epoch 45/50\n",
            "32786/32786 [==============================] - 24s 728us/step - loss: 1.0327 - acc: 0.6322 - val_loss: 2.7950 - val_acc: 0.3365\n",
            "Epoch 46/50\n",
            "32786/32786 [==============================] - 24s 731us/step - loss: 1.0265 - acc: 0.6304 - val_loss: 2.8468 - val_acc: 0.3365\n",
            "Epoch 47/50\n",
            "32786/32786 [==============================] - 24s 725us/step - loss: 0.9961 - acc: 0.6420 - val_loss: 2.9002 - val_acc: 0.3360\n",
            "Epoch 48/50\n",
            "32786/32786 [==============================] - 24s 730us/step - loss: 0.9894 - acc: 0.6484 - val_loss: 2.9887 - val_acc: 0.3363\n",
            "Epoch 49/50\n",
            "32786/32786 [==============================] - 24s 723us/step - loss: 0.9697 - acc: 0.6535 - val_loss: 3.0009 - val_acc: 0.3343\n",
            "Epoch 50/50\n",
            "32786/32786 [==============================] - 24s 728us/step - loss: 0.9467 - acc: 0.6621 - val_loss: 3.0546 - val_acc: 0.3371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6af234d6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsTQ4wmcYkeP",
        "colab_type": "code",
        "outputId": "90f309b7-9028-4dec-c12f-9a061ef673c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "v = Input(shape=(1, 4, 32, 32))\n",
        "x = Conv3D(25, (2,3,3), activation = 'relu', data_format='channels_first')(v)\n",
        "x = Conv3D(15, 2, activation = 'relu')(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(11, activation = 'softmax')(x)\n",
        "net4 = Model(v,x)\n",
        "net4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "net4.summary()\n",
        "net4.fit(X_train,y_train,epochs=50,batch_size=50,shuffle=True, validation_data=(X_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_18 (InputLayer)        (None, 1, 4, 32, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_20 (Conv3D)           (None, 25, 3, 30, 30)     475       \n",
            "_________________________________________________________________\n",
            "conv3d_21 (Conv3D)           (None, 24, 2, 29, 15)     3615      \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 20880)             0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 100)               2088100   \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 32)                3232      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 2,095,785\n",
            "Trainable params: 2,095,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 32786 samples, validate on 8197 samples\n",
            "Epoch 1/50\n",
            "32786/32786 [==============================] - 14s 442us/step - loss: 1.9014 - acc: 0.3231 - val_loss: 1.8031 - val_acc: 0.3404\n",
            "Epoch 2/50\n",
            "32786/32786 [==============================] - 13s 398us/step - loss: 1.7841 - acc: 0.3376 - val_loss: 1.7949 - val_acc: 0.3376\n",
            "Epoch 3/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 1.7632 - acc: 0.3408 - val_loss: 1.7532 - val_acc: 0.3470\n",
            "Epoch 4/50\n",
            "32786/32786 [==============================] - 13s 395us/step - loss: 1.7494 - acc: 0.3423 - val_loss: 1.7383 - val_acc: 0.3401\n",
            "Epoch 5/50\n",
            "32786/32786 [==============================] - 13s 394us/step - loss: 1.7364 - acc: 0.3457 - val_loss: 1.7507 - val_acc: 0.3415\n",
            "Epoch 6/50\n",
            "32786/32786 [==============================] - 13s 399us/step - loss: 1.7262 - acc: 0.3484 - val_loss: 1.7471 - val_acc: 0.3394\n",
            "Epoch 7/50\n",
            "32786/32786 [==============================] - 13s 402us/step - loss: 1.7270 - acc: 0.3476 - val_loss: 1.7354 - val_acc: 0.3416\n",
            "Epoch 8/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.7134 - acc: 0.3524 - val_loss: 1.7420 - val_acc: 0.3411\n",
            "Epoch 9/50\n",
            "32786/32786 [==============================] - 13s 398us/step - loss: 1.7098 - acc: 0.3550 - val_loss: 1.7418 - val_acc: 0.3388\n",
            "Epoch 10/50\n",
            "32786/32786 [==============================] - 13s 395us/step - loss: 1.7015 - acc: 0.3607 - val_loss: 1.7361 - val_acc: 0.3409\n",
            "Epoch 11/50\n",
            "32786/32786 [==============================] - 13s 394us/step - loss: 1.6907 - acc: 0.3635 - val_loss: 1.7480 - val_acc: 0.3404\n",
            "Epoch 12/50\n",
            "32786/32786 [==============================] - 13s 398us/step - loss: 1.6809 - acc: 0.3686 - val_loss: 1.7525 - val_acc: 0.3434\n",
            "Epoch 13/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.6700 - acc: 0.3754 - val_loss: 1.7533 - val_acc: 0.3409\n",
            "Epoch 14/50\n",
            "32786/32786 [==============================] - 13s 403us/step - loss: 1.6604 - acc: 0.3785 - val_loss: 1.7667 - val_acc: 0.3421\n",
            "Epoch 15/50\n",
            "32786/32786 [==============================] - 13s 393us/step - loss: 1.6446 - acc: 0.3868 - val_loss: 1.7821 - val_acc: 0.3381\n",
            "Epoch 16/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 1.6334 - acc: 0.3949 - val_loss: 1.8041 - val_acc: 0.3406\n",
            "Epoch 17/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 1.6128 - acc: 0.4024 - val_loss: 1.8119 - val_acc: 0.3385\n",
            "Epoch 18/50\n",
            "32786/32786 [==============================] - 13s 395us/step - loss: 1.5948 - acc: 0.4099 - val_loss: 1.8269 - val_acc: 0.3362\n",
            "Epoch 19/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.5753 - acc: 0.4197 - val_loss: 1.8523 - val_acc: 0.3412\n",
            "Epoch 20/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.5573 - acc: 0.4235 - val_loss: 1.8815 - val_acc: 0.3371\n",
            "Epoch 21/50\n",
            "32786/32786 [==============================] - 13s 398us/step - loss: 1.5289 - acc: 0.4370 - val_loss: 1.9241 - val_acc: 0.3341\n",
            "Epoch 22/50\n",
            "32786/32786 [==============================] - 13s 394us/step - loss: 1.5041 - acc: 0.4468 - val_loss: 1.9871 - val_acc: 0.3391\n",
            "Epoch 23/50\n",
            "32786/32786 [==============================] - 13s 395us/step - loss: 1.4822 - acc: 0.4591 - val_loss: 1.9749 - val_acc: 0.3344\n",
            "Epoch 24/50\n",
            "32786/32786 [==============================] - 13s 393us/step - loss: 1.4542 - acc: 0.4654 - val_loss: 2.0489 - val_acc: 0.3413\n",
            "Epoch 25/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.4285 - acc: 0.4755 - val_loss: 2.0710 - val_acc: 0.3345\n",
            "Epoch 26/50\n",
            "32786/32786 [==============================] - 13s 395us/step - loss: 1.4006 - acc: 0.4875 - val_loss: 2.1329 - val_acc: 0.3329\n",
            "Epoch 27/50\n",
            "32786/32786 [==============================] - 13s 395us/step - loss: 1.3743 - acc: 0.4965 - val_loss: 2.1931 - val_acc: 0.3354\n",
            "Epoch 28/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 1.3471 - acc: 0.5036 - val_loss: 2.1995 - val_acc: 0.3333\n",
            "Epoch 29/50\n",
            "32786/32786 [==============================] - 13s 400us/step - loss: 1.3275 - acc: 0.5151 - val_loss: 2.2678 - val_acc: 0.3383\n",
            "Epoch 30/50\n",
            "32786/32786 [==============================] - 13s 398us/step - loss: 1.2925 - acc: 0.5267 - val_loss: 2.3115 - val_acc: 0.3398\n",
            "Epoch 31/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.2682 - acc: 0.5344 - val_loss: 2.4009 - val_acc: 0.3355\n",
            "Epoch 32/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 1.2440 - acc: 0.5419 - val_loss: 2.4706 - val_acc: 0.3387\n",
            "Epoch 33/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.2163 - acc: 0.5545 - val_loss: 2.4958 - val_acc: 0.3334\n",
            "Epoch 34/50\n",
            "32786/32786 [==============================] - 13s 391us/step - loss: 1.1888 - acc: 0.5643 - val_loss: 2.5689 - val_acc: 0.3409\n",
            "Epoch 35/50\n",
            "32786/32786 [==============================] - 13s 399us/step - loss: 1.1625 - acc: 0.5738 - val_loss: 2.6738 - val_acc: 0.3382\n",
            "Epoch 36/50\n",
            "32786/32786 [==============================] - 13s 398us/step - loss: 1.1413 - acc: 0.5791 - val_loss: 2.7433 - val_acc: 0.3389\n",
            "Epoch 37/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 1.1196 - acc: 0.5889 - val_loss: 2.7722 - val_acc: 0.3396\n",
            "Epoch 38/50\n",
            "32786/32786 [==============================] - 13s 401us/step - loss: 1.0945 - acc: 0.5996 - val_loss: 2.8524 - val_acc: 0.3362\n",
            "Epoch 39/50\n",
            "32786/32786 [==============================] - 13s 400us/step - loss: 1.0742 - acc: 0.6043 - val_loss: 2.9160 - val_acc: 0.3382\n",
            "Epoch 40/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.0533 - acc: 0.6135 - val_loss: 3.0072 - val_acc: 0.3368\n",
            "Epoch 41/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 1.0343 - acc: 0.6187 - val_loss: 3.0466 - val_acc: 0.3402\n",
            "Epoch 42/50\n",
            "32786/32786 [==============================] - 13s 399us/step - loss: 1.0046 - acc: 0.6290 - val_loss: 3.1563 - val_acc: 0.3395\n",
            "Epoch 43/50\n",
            "32786/32786 [==============================] - 13s 397us/step - loss: 1.0036 - acc: 0.6291 - val_loss: 3.1669 - val_acc: 0.3401\n",
            "Epoch 44/50\n",
            "32786/32786 [==============================] - 13s 398us/step - loss: 0.9725 - acc: 0.6416 - val_loss: 3.2383 - val_acc: 0.3368\n",
            "Epoch 45/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 0.9493 - acc: 0.6507 - val_loss: 3.3176 - val_acc: 0.3351\n",
            "Epoch 46/50\n",
            "32786/32786 [==============================] - 13s 396us/step - loss: 0.9341 - acc: 0.6549 - val_loss: 3.4499 - val_acc: 0.3379\n",
            "Epoch 47/50\n",
            "32786/32786 [==============================] - 13s 398us/step - loss: 0.9210 - acc: 0.6618 - val_loss: 3.5326 - val_acc: 0.3285\n",
            "Epoch 48/50\n",
            "32786/32786 [==============================] - 13s 393us/step - loss: 0.9069 - acc: 0.6680 - val_loss: 3.5332 - val_acc: 0.3420\n",
            "Epoch 49/50\n",
            "32786/32786 [==============================] - 13s 403us/step - loss: 0.8759 - acc: 0.6761 - val_loss: 3.6019 - val_acc: 0.3343\n",
            "Epoch 50/50\n",
            "32786/32786 [==============================] - 13s 400us/step - loss: 0.8640 - acc: 0.6808 - val_loss: 3.7695 - val_acc: 0.3334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6af226bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XJY_ONmZXL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}